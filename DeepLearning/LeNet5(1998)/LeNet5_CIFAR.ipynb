{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Settings\n",
    "\n",
    "#Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "#Architecture\n",
    "NUM_FEATURES = 32*32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "#other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Image batch dimensions: torch.Size([128, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "######CIFAR-10 DATASET\n",
    "\n",
    "\n",
    "#Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='data',\n",
    "                                 train=True,\n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "test_dataset = datasets.CIFAR10(root='data',\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1| Batch index: 0| Batch size: 128\n",
      "Epoch: 2| Batch index: 0| Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print('| Batch index:', batch_idx, end='')\n",
    "        print('| Batch size:', y.size()[0])\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######MODEL\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes, grayscale=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        self.grayscale = grayscale\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if self.grayscale:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 6*in_channels, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(6*in_channels, 16*in_channels, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5*in_channels, 120*in_channels),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120*in_channels, 84*in_channels),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84*in_channels, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.classifier(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = LeNet5(NUM_CLASSES, GRAYSCALE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 1.4406\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 1.1846\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 1.3550\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 1.1597\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 1.2673\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 1.0845\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 1.1204\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 1.1967\n",
      "Epoch: 001/010 | Train: 61.574%\n",
      "Time elapsed: 1.35 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 1.1968\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 1.1385\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 1.0847\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 1.0360\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 1.1584\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 1.2345\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 1.2102\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 1.0717\n",
      "Epoch: 002/010 | Train: 65.570%\n",
      "Time elapsed: 2.59 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 1.0155\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 1.0971\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 1.0100\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.9906\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 1.1694\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 1.0316\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 1.2017\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 0.9893\n",
      "Epoch: 003/010 | Train: 68.936%\n",
      "Time elapsed: 3.86 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.9150\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 1.0452\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 0.7626\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 0.8670\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 0.9321\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 0.8469\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 1.0102\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 0.7633\n",
      "Epoch: 004/010 | Train: 70.998%\n",
      "Time elapsed: 5.17 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.7881\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.8576\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 0.8087\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 0.8271\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.8995\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.7499\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 0.9647\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.9170\n",
      "Epoch: 005/010 | Train: 75.862%\n",
      "Time elapsed: 6.45 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.7357\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.8792\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.7033\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.6526\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.7671\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.8369\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.9223\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.8644\n",
      "Epoch: 006/010 | Train: 78.336%\n",
      "Time elapsed: 7.74 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.5908\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.7039\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.6208\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.5054\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.5254\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.7858\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.5643\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.6310\n",
      "Epoch: 007/010 | Train: 81.486%\n",
      "Time elapsed: 9.10 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.5849\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.4411\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.6064\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.6194\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.5501\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.5417\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.5414\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.6410\n",
      "Epoch: 008/010 | Train: 86.342%\n",
      "Time elapsed: 10.40 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.4500\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.4174\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.5098\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.5409\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.3895\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.5902\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.3700\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.5544\n",
      "Epoch: 009/010 | Train: 89.176%\n",
      "Time elapsed: 11.63 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.3464\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.2574\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.4499\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.2840\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.3250\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.3789\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.3456\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.3775\n",
      "Epoch: 010/010 | Train: 90.980%\n",
      "Time elapsed: 12.89 min\n",
      "Total Training Time: 12.89 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "        ###LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' %(epoch+1, NUM_EPOCHS, batch_idx, len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): #save during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%'%(epoch+1, NUM_EPOCHS, compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "    print('Time elapsed: %.2f min'%((time.time()-start_time)/60))\n",
    "\n",
    "print('Total Training Time: %.2f min'%((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 66.36%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "    print('Test accuracy: %.2f%%'%(compute_accuracy(model, test_loader, device=DEVICE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
